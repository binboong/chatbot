import os
import sys
import logging
import chromadb
from chromadb.utils import embedding_functions
from chromadb.config import Settings
import google.generativeai as genai
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException
from fastapi.responses import JSONResponse
from pydantic import BaseModel, validator
from typing import Optional
from onnx_embedder import get_embedder   # import file onnx_embedder.py c·ªßa b·∫°n

# Logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)

# UTF-8 cho console
if sys.stdout.encoding != "utf-8":
    sys.stdout.reconfigure(encoding="utf-8")
if sys.stderr.encoding != "utf-8":
    sys.stderr.reconfigure(encoding="utf-8")

# ===== Gemini Init =====
try:
    load_dotenv()
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        raise ValueError("Missing GOOGLE_API_KEY")

    genai.configure(api_key=api_key)
    llm = genai.GenerativeModel("gemini-2.5-flash-lite")
    logger.info("‚úÖ Gemini initialized")
except Exception as e:
    logger.error(f"‚ùå Gemini init failed: {e}")
    sys.exit(1)

# ===== ONNX Embedder =====
try:
    logger.info("üîÑ Loading ONNX embedder...")
    embedder = get_embedder("./models")
    logger.info("‚úÖ ONNX embedder ready")
except Exception as e:
    logger.error(f"‚ùå ONNX embedder failed: {e}")
    sys.exit(1)

# Custom embedding function cho ChromaDB
class ONNXEmbeddingFunction(embedding_functions.EmbeddingFunction):
    def __init__(self, model_path="./models"):
        self.embedder = get_embedder(model_path)

    def __call__(self, input):
        if isinstance(input, str):
            input = [input]
        embeddings = self.embedder(input)
        return embeddings.tolist() if hasattr(embeddings, "tolist") else embeddings

# ===== ChromaDB Init =====
try:
    client = chromadb.PersistentClient(
        path="./vector_db",
        settings=Settings(anonymized_telemetry=False, allow_reset=True)
    )

    embedding_fn = ONNXEmbeddingFunction(model_path="./models")
    collection = client.get_or_create_collection(
        name="knowledge_base",
        embedding_function=embedding_fn,
        metadata={"embedding_function": "onnx_custom"}
    )
    logger.info(f"‚úÖ ChromaDB ready. Docs: {collection.count()}")
except Exception as e:
    logger.error(f"‚ùå ChromaDB failed: {e}")
    sys.exit(1)

# ===== FastAPI App =====
app = FastAPI(
    title="Chatbot API",
    description="Vietnamese-English Chatbot with ONNX RAG",
    version="2.0.0"
)

@app.middleware("http")
async def ensure_utf8(request, call_next):
    response = await call_next(request)
    if hasattr(response, "headers"):
        response.headers["Content-Type"] = "application/json; charset=utf-8"
    return response

# ===== Schemas =====
class Query(BaseModel):
    user_id: str
    message: str
    max_results: Optional[int] = 5

    @validator("message")
    def message_not_empty(cls, v):
        if not v or not v.strip():
            raise ValueError("Message cannot be empty")
        return v.strip()

    @validator("user_id")
    def user_id_not_empty(cls, v):
        if not v or not v.strip():
            raise ValueError("User ID cannot be empty")
        return v.strip()

class ChatResponse(BaseModel):
    reply: str
    sources: Optional[list] = []
    user_id: str
    success: bool = True

# ===== Exception Handler =====
@app.exception_handler(Exception)
async def global_exception_handler(request, exc):
    logger.error(f"Unhandled exception: {exc}")
    return JSONResponse(
        status_code=500,
        content={
            "reply": "ƒê√£ x·∫£y ra l·ªói. Vui l√≤ng th·ª≠ l·∫°i.",
            "success": False,
            "error": str(exc)
        }
    )

# ===== Endpoints =====
@app.get("/")
async def root():
    return {
        "message": "ü§ñ Chatbot API (ONNX) is running!",
        "version": "2.0.0",
        "engine": "ONNX Runtime (lightweight)",
        "endpoints": {
            "chat": "POST /chat",
            "health": "GET /health",
            "stats": "GET /stats"
        }
    }

@app.get("/health")
async def health():
    try:
        collection.count()
        test_response = llm.generate_content("test")
        return {
            "status": "healthy",
            "chromadb": "connected",
            "gemini": "connected",
            "embedder": "onnx",
            "documents": collection.count()
        }
    except Exception as e:
        logger.error(f"Health check failed: {e}")
        raise HTTPException(status_code=503, detail=str(e))

@app.post("/chat", response_model=ChatResponse)
async def chat(query: Query):
    try:
        logger.info(f"Chat from {query.user_id}: {query.message[:100]}...")

        # Search trong Chroma
        results = collection.query(
            query_texts=[query.message],
            n_results=min(query.max_results, 30),  # TƒÉng l√™n 15
            include=["documents", "metadatas", "distances"]  # Th√™m distances ƒë·ªÉ filter
        )

        context = ""
        sources = []
        if results["documents"] and results["documents"][0]:
            docs = results["documents"][0]
            distances = results.get("distances", [[]])[0]  # Th√™m d√≤ng n√†y
            
            for i, doc in enumerate(docs):
                # S·ª≠a logic: distance < 0.7 m·ªõi l√† relevant (kh√¥ng ph·∫£i > 0.7)
                for i, doc in enumerate(docs):
                    if distances and len(distances) > i:
                        print(f"üìä Doc {i}: distance={distances[i]:.4f}")  # Debug xem distance th·ª±c t·∫ø
                    
                    if doc and doc.strip():
                        context += doc + "\n\n"
                        # ... rest
                    if (results.get("metadatas") and 
                        results["metadatas"][0] and 
                        len(results["metadatas"][0]) > i):
                        source = results["metadatas"][0][i].get("source", "Unknown")
                        if source not in sources:
                            sources.append(source)

            if len(context) > 12000:
                context = context[:12000] + "..."

        if not context.strip():
            context = "Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu li√™n quan."

        prompt = f"""
B·∫°n l√† tr·ª£ l√Ω AI th√¥ng minh v√† th√¢n thi·ªán. Tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n th√¥ng tin. Th√¥ng tin m√† b·∫°n ƒë∆∞·ª£c cung c·∫•p c√≥ th·ªÉ bao g·ªìm c√°c tr√≠ch d·∫´n t·ª´ c√°c ngu·ªìn kh√°c nhau.
Ngu·ªìn th√¥ng tin c·ªßa b·∫°n l√† ƒë·∫øn t·ª´ c√°c ticket customer support v√† c√°c t√†i li·ªáu k·ªπ thu·∫≠t v·ªÅ s·∫£n ph·∫©m c·ªßa c√¥ng ty.
C√≥ th·ªÉ th√¥ng tin n√†y kh√¥ng ƒë·∫ßy ƒë·ªß ho·∫∑c kh√¥ng ch√≠nh x√°c v√† c≈©ng c√≥ th·ªÉ c√≥ r·∫•t nhi·ªÅu th√¥ng tin b·ªã th·ª´a th√£i b√™n trong c∆° s·ªü d·ªØ li·ªáu (nh∆∞ l√† email, t√™n ng∆∞·ªùi g·ª≠i, ƒë·ªãa ch·ªâ, etc...). H√£y t·ª± c√¢n nh·∫Øc.
N·∫øu b·∫°n kh√¥ng ch·∫Øc ch·∫Øn v·ªÅ c√¢u tr·∫£ l·ªùi, h√£y n√≥i th·∫≥ng r·∫±ng b·∫°n kh√¥ng bi·∫øt thay v√¨ ƒëo√°n m√≤.
(You are a smart and friendly AI assistant. Answer questions based on provided information. The information you are given may include excerpts from various sources.
Your information sources come from customer support tickets and technical documents about the company's products.)

TH√îNG TIN (Information):
{context}

NG∆Ø·ªúI D√ôNG (User) ({query.user_id}): {query.message}

H∆Ø·ªöNG D·∫™N:
1. V·ªÅ ng√¥n ng·ªØ (Language): Tr·∫£ l·ªùi l·∫°i c√πng ng√¥n ng·ªØ v·ªõi ng∆∞·ªùi d√πng. (Match user's language, Vietnamese, English, Japanese, Chinese, etc.)
2. D·ª±a v√†o th√¥ng tin c√≥ s·∫µn (Based on available info)
3. N·∫øu kh√¥ng c√≥ th√¥ng tin, n√≥i th·∫≥ng (If no info, say so directly)
4. Ng·∫Øn g·ªçn, r√µ r√†ng, h·ªØu √≠ch (Concise, clear, helpful)
5. UTF-8 encoding ƒë√∫ng (Proper UTF-8 encoding)

TR·∫¢ L·ªúI (Answer):
"""

        response = llm.generate_content(prompt)
        if not response or not response.text:
            raise Exception("Empty Gemini response")

        reply_text = response.text.strip()
        reply_text = reply_text.encode("utf-8").decode("utf-8")

        logger.info(f"‚úÖ Response generated for {query.user_id}")

        return {
            "reply": reply_text,
            "sources": sources,
            "user_id": query.user_id,
            "success": True
        }

    except ValueError as ve:
        logger.error(f"Validation error: {ve}")
        raise HTTPException(status_code=400, detail=str(ve))
    except Exception as e:
        logger.error(f"Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/stats")
async def stats():
    try:
        count = collection.count()
        return {
            "total_documents": count,
            "collection_name": collection.name,
            "engine": "ONNX Runtime",
            "status": "active"
        }
    except Exception as e:
        logger.error(f"Stats error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ===== Run with Uvicorn =====
if __name__ == "__main__":
    import uvicorn
    logger.info("Starting API server...")
    uvicorn.run("api:app", host="0.0.0.0", port=8000, reload=False)
